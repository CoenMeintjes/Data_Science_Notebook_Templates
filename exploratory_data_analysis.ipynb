{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xSDP4bMrIL2x"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   **[Introduction to Gradient Boosting](#1.-Introduction-to-Gradient-Boosting)**\n",
    "2.   **[Foundations of Gradient Boosting](#2.-Foundations-of-Gradient-Boosting)**\n",
    "3.   **[Gradient Boost Tuning](#3.-Gradient-Boost-Tuning)**\n",
    "4.   **[Model Validation](#4.-Model-Validation)**\n",
    "5.   **[Exploratory Data Analysis](#5.-Exploratory-Data-Analysis)**\n",
    "6.   **[Model Construction](#6.-Model-Construction)**\n",
    "7.   **[Model Evaluation](#7.-Model-Evaluation)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"1.-Introduction--to-EDA\"></a>\n",
    "### 1. Introduction to EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Foundations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis (EDA) |** The process of investigating, organizing, and analyzing datasets and summarizing their main characteristics, often employing data wrangling and visualization methods.\n",
    "\n",
    "**6 Practices of EDA:**\n",
    "- **Discovering |** process of data familiarization in order to conceptualize how the data can be used\n",
    "- **Structuring |** the process of taking raw data and organizing or transforming it to be more easily visualized, explained, or modeled \n",
    "- **Cleaning |** the process of removing errors that may distort your data or make it less useful\n",
    "- **Joining |** the process of augmenting or adjusting data by adding values from other datasets\n",
    "- **Validating |** the process of verifying that the data is consistent and high quality\n",
    "- **Presenting |** making the cleaned dataset or data visualizations available to others for analysis or further modeling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"5.-Exploratory-Data-Analysis\"></a>\n",
    "### 5. Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and modules.\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset into a DataFrame and save in a variable\n",
    "data = pd.read_csv(\"example_file.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Data Exploration\n",
    "After loading the dataset, the next step is to prepare the data to be suitable for clustering. This includes: \n",
    "\n",
    "*   Exploring data\n",
    "*   Checking for missing values\n",
    "*   Encoding categorical data \n",
    "*   Dropping irrelevant columns\n",
    "*   Renaming columns\n",
    "*   Create training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of rows, number of columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data type for each column. NB logistic regression models expect numeric data\n",
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to answer:** Identify the target (or predicted) variable. What is the initial hypothesis about which variables will be valuable in predicting the target variable?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3 Prepare model for predictions\n",
    "\n",
    "**Question to answer:** Before proceeding with modeling, consider which metrics should ultimately be leveraged to evaluate the model. Which metrics are most suited to evaluating this type of model?\n",
    "- Important to evaluate not just accuracy but the balance of false positives and false negatives that the model predicts. Therefore precision, recall and f1 score will be the best metrics for classification models.\n",
    "- The ROC AUC score is also suited to classification modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.1 Convert Variables to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the object predictor variables to numerical dummies.\n",
    "data_dummies = pd.get_dummies(data, \n",
    "                                columns=['categorical_column1','categorical_column2','categorical_column3','categorical_column4'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.2 Isolate Target and Predictor Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset into labels (y) and features (X).\n",
    "y = data_subset[\"target_variable\"]\n",
    "\n",
    "X = data_subset.copy()\n",
    "X = X.drop(\"target_variable\", axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.4 Create Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into train, validate, test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify= y, random_state = 0)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.25, stratify= y_train, random_state = 0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1BgN3Lv1fx-AxyKSqB_2kM3dJ4mFBctv_",
     "timestamp": 1662734078308
    },
    {
     "file_id": "1ZYfhIvPRxnw7ghB_BsNQAMUorLXpAZs_",
     "timestamp": 1658889786811
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
